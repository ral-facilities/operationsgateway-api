# EPAC Simulated Data

This directory contains a number of scripts (and assisting Python code) to generate and ingest simulated data. This will allow us to have a more effective test & demo platform as the data should be closer to real data than the small amount of Gemini data we have access to. We hope to have one year of simulated data, and data generated each day to simulate an incoming load from the facility.

The data is originally generated by a tool made by CLF, [EPAC-DataSim](https://github.com/CentralLaserFacility/EPAC-DataSim). The outputs from this tool are used in the scripts which form the following process:

1. Convert yaml files that store the data's schedule/experiments into a format that can be imported straight into MongoDB (done by `generation.sh`, files generated by EPAC DataSim beforehand)
2. Generate HDF files using EPAC-DataSim (done by `generation.sh`)
3. Upload HDF files and `resources/` to Echo (done by `generation.sh`)
4. Ingest channel manifest file for this data and import experiments into MongoDB (manifest ingested via API endpoint, experiments imported via `mongoimport`)
5. Download HDF files from Echo and ingest them into the API
6. Ingest HDF files generated locally on a daily basis to simulate incoming load from EPAC

To generate a year's worth of data that's now stored in Echo (plus a supporting file for daily data generation), the following commands were used:
```bash
# Creates `schedule_calendar.yml`
# You need Python 3.9 for this otherwise you'll face a `ModuleNotFoundError` for `zoneinfo`. Use `pyenv` or install a different version using `yum`
poetry run epac-data-sim calendar -s 2022-06-01 -e 2023-06-01 -o ~/dev/operationsgateway-api/util/realistic_data/resources/

# Creates a second calendar file which is renamed as a schedule for daily data
poetry run epac-data-sim calendar -s 2023-06-02 -e 2027-01-01 -o /tmp
mv /tmp/schedule_calendar.yml ~/dev/operationsgateway-api/util/realistic_data/resources/daily_data_schedule.yml

# Clone EPAC-DataSim and create virtualenv so we have access to config generator script
cd ..; git clone git@github.com:CentralLaserFacility/EPAC-DataSim.git
# Script that is used to generate the two files requires Python 3.10 due to match/case. If you're using pyenv, use `pyenv local 3.10`
cd EPAC-DataSim; python -m venv simulated-data-310
source simulated-data-310/bin/activate
pip install pyproject.toml pyyaml

# After running the script, I used https://jsonformatter.org/json-pretty-print to turn the single line manifest file into JSON pretty print for readability
python tools/gen-config.py -o ~/dev/operationsgateway-api/util/realistic_data/resources/channels_config.yml -m ~/dev/operationsgateway-api/util/realistic_data/resources/channel_manifest.json
# We don't need the virtual environment any more
deactivate; cd ~/dev/operationsgateway-api

# Run Bash script that generates a day of data at a time and uploads it to Echo
# The script also takes the calendar files etc. and uploads them to Echo
# EDIT VARIABLES AT TOP OF THE SCRIPT TO APPROPRIATE VALUES BEFORE RUNNING
# If the process uploading HDF files gets killed, it's probably run out of memory. Decreasing the number of threads seems to help - on my dev VM, I used 6 threads (`s4cmd` default is 16). Keep an eye on resources using `htop` and tweak as needed
mkdir ~/dev/operationsgateway-api/util/realistic_data/data
# `nohup` turns this into a background process, storing output in `data_generation.log`
nohup ./util/realistic_data/generation.sh >> /home/xfu59478/dev/operationsgateway-api/data_generation.log 2>&1 &
```

To ingest the data from Echo and setup daily data ingestion, run OG Ansible (which gets everything in place and creates cron jobs for the daily data) and run the following commands on the dev server:
```bash
# If you don't have the API running already and the `launch_api` option is set to false, start up an instance. `-t` increases the time allowed for workers to respond to a request, handy for these bigger HDF files
poetry run gunicorn operationsgateway_api.src.main:app --workers 2 --worker-class uvicorn.workers.UvicornWorker -b 127.0.0.1:8000 -t 100

# Set appropriate config in util/realistic_data/config.yml beforehand
poetry run python util/realistic_data/ingest_echo_data.py
```

## Calendar Conversion
This is a script that converts a schedule generated by EPAC-DataSim (see [example schedule](https://github.com/CentralLaserFacility/EPAC-DataSim/blob/development/schedule_config.yml)) and puts it in a format that works with the `experiments` collection found in OperationsGateway's database. This conversion needs to happen because the API's way of getting experiments is via the Scheduler (software hosted by ISIS). The data generated using CLF's tool doesn't have any related experiments on the Scheduler, therefore we must import our own straight into the database.

This script writes a file called `experiments_for_mongoimport.json` which contains multiple JSON objects that MongoDB will use to put the data into the database.

This script can take multiple schedule files as inputs and generate a single mongoimport file. This ability is needed as we have multiple schedule inputs for the dev server - a file for the Echo data and a file for the 'daily data'.

This script is used in `generation.sh` so look in there for an example command.

## Generation
This bash script is used to generate a configurable amount of data and store it in Echo. The amount of data that is generated can be changed by adjusting the number of days to generate and what the start date should be for data generation. Use the `date` and `num_days_to_generate` variables at the top of the file. When deciding what these should be, make sure the data generated will fall inside the schedule - EPAC-DataSim won't generate any data if it's asked to generate outside a schedule. Also, be sure adjust the variables storing paths and bucket name to suit.

The script generates and uploads data in day increments. Using realistic channel configuration, a day of data can be around 44GB so daily increments seem sensible in order to not fill up a hard disk.

As well as uploading the HDF files, the script will also upload a handful of supporting files, stored in a 'resources' directory.

### Resources Directory
The resources directory is used in a number of places throughout the simulated data (as well as being stored in Echo) and should contain the following files:
- Channels config file (used when generating data with EPAC-DataSim)
- Channel manifest file (same channels as the channels config, ingested into the API via `ingest_echo_data.py`)
- Schedule calendar file (used when generating data with EPAC-DataSim to know when's an experiment day etc.)
- Experiments for mongoimport (a version of the schedule calendar file which is used to import experiments into MongoDB). This file is generated by the calendar conversion script, ran by `generation.sh` so this file isn't needed at the start of the process
- Daily data schedule file (similar to the schedule calendar file but used for daily data). The end date of this file defines when the daily data will stop working - currently set for 2027 which should outlive the need for simulated data

## Ingest Echo Data
This script performs a similar perform to the older ingestion script (`util/ingest_hdf.py`) by ingesting a series of HDF files and associated channel manifest file. This script also imports experiments related to the simulated data as they cannot be retrieved from the Scheduler.

The old script has been kept around in case we need to ingest the older Gemini test data that we have access to. We've been advised by CLF that there may be cases where looking at real data (despite it being Gemini data) might be preferable to the EPAC simulated data.

Instead of grabbing them from a GitHub repo, data (both HDF and metadata files such as the manifest file) is now downloaded from Echo. A `ThreadPool` is used to deal with the large amount of HDF files - when a year's worth of data is stored in Echo, this will be 175,200 HDF files. The pool has the same number of threads as there are configured Gunicorn workers.

Given the large amount of files there will be to download and ingest, I'd suggest that this command is left to run overnight (or perhaps even several nights!).

### Config
The ingestion script has a large number of configuration parameters needed to run and as such, it has its own config file (`util/realistic_data/config.yml`) instead of using something like `ArgumentParser` - the command for running the script would be huge otherwise! The script requires credentials for Echo which would be appropriate to give via a command as the credentials would be stored in `history`.

There's an example file to use - copy that to `config.yml` and edit the examples as needed. For performance, tweaking `page_size` and `gunicorn_num_workers` might be needed - I found that on a cloud machine with huge amounts of CPU and RAM, values of 480 and 120 respectively worked best but it depends massively on the specs of the machine running the script.

### Ingest Module
To run the script, there are a handful of classes stored in `util/realistic_data/ingest/` which relate to various aspects of the script's functionality. Separating this code out from the main script makes it easier to maintain the code and hopefully makes the script fairly self-documenting (as its just calling other bits of code which are well-named).

## Daily Ingestor
Effectively the final script in the pipeline of simulated data, this script takes a directory of HDF files (as a command line argument) and ingests that are timestamped older than the current datetime. This is done via the filename (e.g. `2023-07-31T1024.h5`) which is written in UTC.

If the ingestion is successful, the HDF file is deleted (as we no longer need it, all the data is in MongoDB/Echo at this point) but if the ingestion fails, the file is moved into a 'failed ingestion' directory (also given to the script as a command line argument). This allows developers to check on the machine periodically and easily seeing which files have failed to ingest and debug the issue appropriately.

With two cron jobs (set up by the Ansible), we can ensure that the files are being ingested at the correct time, simulating realtime ingestion. The first cron job generates a day's worth of files at 22:00 (so it generates the following day of data, and 22:00 allows for any differences in timezones between the machine and the UK). The second job runs the daily ingestor script every minute which allows files to be regularly ingested - there is no expectation that we will recieve multiple files per minute so this frequency is sufficient.
