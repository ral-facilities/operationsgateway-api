# Test Data
In OperationsGateway, there are two sources of test data:

1. Data from Gemini stored in [operationsgateway-test-data](https://github.com/ral-facilities/operationsgateway-test-data)
2. Simulated data stored in Echo S3

In the early stages of the API, a small amount of data was manually exported from the Gemini instance of eCat (using the system's export functionality) and converted into HDF files using [OG-HDF5](https://github.com/CentralLaserFacility/OG-HDF5). The files were stored in [operationsgateway-test-data](https://github.com/ral-facilities/operationsgateway-test-data) and contains a number of files of shot data as well as environmental. The README of that repo details how the files are separated into different directories and what each 'set' contains.

This was suitable as a starting point but wasn't entirely representative of the data from EPAC and the data volume wasn't large enough to test performance. The API's tests now use simulated data as this data is more realistic. There may be cases where we need to revert back to the old Gemini data for manual testing on the dev server, `dev_server.md` has a section dedicated to explain how this would work (we haven't had a situation where we needed to revert back as of yet).

The simulated data are the HDF files generated by [EPAC-DataSim](https://github.com/CentralLaserFacility/EPAC-DataSim), a tool provided to us by CLF to give us data realistic to that of the data which we will receive from EPAC in production. The repo's README provides a good explanation of how to use it. The HDF files generated when there's an experiment are quite large (~300MB). As a result only a few shot files are part of the simulated data used for tests and some environmental data too - this helps keep CI times to a reasonable amount. This is a very small subset of the data stored on the dev server (i.e. the year of simulated data).

## Using Gemini Data
To load Gemini data into your development environment, the `ingest_hdf.py` script needs to be used. The OperationsGateway Test Data repo also needs to be cloned on your machine beforehand.

```bash
# This is a private repo so ensure you have permissions to access it and your SSH key linked with your GitHub account is loaded on the machine
git clone git@github.com:ral-facilities/operationsgateway-test-data.git

# Run ingest_hdf.py script from the base level of this repo, specifying a number of options
poetry run python util/ingest_hdf.py -p ../operationsgateway-test-data/dev_server/ -w -d -U backend -P back -e -j util/users_for_mongoimport.json
```

The script has a number of options that need to be specified:
- `-p [PATH TO OPERATIONSGATEWAY-TEST-DATA]` - provides the path to the Gemini test data
- `-w` - wipes specific collections in the database to previous data doesn't impact the tests
- `-d` - deletes files stored in Echo (the bucket name is retrieved from the API config so ensure this is set correctly)
- `-U backend -P back` - credentials for the backend user
- `-e` - calls `POST /experiments` to grab the a number of experiments from the dev Scheduler, used for an integration test of `GET /experiments`
- `-j [PATH TO TEST USER FILE]` - provides the path to the JSON file containing test users

There are some additional options but these should be enough for most users. Use `--help` on the script to see further arguments.

Depending on which options are enabled, the script will do the following things:
- Wipe the database and Echo
- Import test users
- Start the API so data can be ingested
- Ingest HDF files as well as a channel manifest file
- Grab experiments from dev Scheduler
- Stop the API


## Using Simulated Data
This data is stored on Echo so ensure you have access to the shared folder in Keeper which contains the credentials, or you already have the credentials in your API config. As there's more moving parts to this process, a config file is used, instead of arguments to a script.

```bash
# Copy the example config if not already done, editing the config is correct, see below for more details
cp util/realistic_data/config.yml.example util/realistic_data/config.yml

# Run the script, it normally takes a few minutes to run
poetry run python util/realistic_data/ingest_echo_data.py
```

The example config contains sensible values to use, but there are a few things to bear in mind:
- `wipe_echo` - this can be good to start from a clean slate but depending on the amount of data in the bucket, it may take a while to delete the data from the bucket. Unless you need to start from an empty bucket, leave this set to `false`
- `launch_api` - when set to `true`, this option will launch an instance of the API within the script, and kill it once ingestion has completed - ensure you have stopped any instances of the API running so there's no address collisions! If set to `false`, the script will use an instance of the API pointing elsewhere
- `ingest_mode` - for development environments, `sequential` is the suggested value. `parallel` is meant for ingestion on a highly-specced machine when ingesting for the dev server
- `file_to_restart_ingestion` - an option used for ingestion onto the dev server. For development environments ingesting data for the tests, leave this blank
- `storage_bucket` - set this to the bucket assigned to you for development. This is where your images and waveforms will be stored
- `access_key`, `secret_key` - ensure these are set to the Echo credentials
- `log_config_path` - ensure this is set correctly for your environment

Depending on which options are enabled, the script will do the following things:
- Wipe database and Echo
- Import test users & test experiments from JSON files
- Start the API so data can be ingested
- Ingest HDF files and a channel manifest file 
- Stop the API

## Reverting from Simulated Data back to Gemini Data
As previously stated, there may be cases where you have simulated data loaded into your development environment and you need to reload the Gemini data. As the script used to load Gemini data contains options to wipe the database and Echo, there should be no additional steps required. Clone the test data repo, run the script and check the tests pass!
